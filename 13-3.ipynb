{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231379b-159a-415c-8e0d-72af24a5bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from pyod.models.gmm import GMM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# from pyod.models.iforest import IForest\n",
    "# from pyod.models.gmm import GMM\n",
    "# from sklearn.svm import OneClassSVM\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Dense\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751ffc6-b120-466e-a941-23c0840b8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_prepare(df):\n",
    "    \"\"\"\n",
    "    Cleans and prepares the dataframe:\n",
    "    - Ensures datetime index\n",
    "    - Handles missing values without affecting outlier detection\n",
    "    - Creates time-based features\n",
    "    - Drops non-numeric data\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure datetime index\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df[\"dt_time\"] = pd.to_datetime(df[\"dt_time\"])\n",
    "        df = df.set_index(\"dt_time\")\n",
    "\n",
    "    # Remove non-relevant columns\n",
    "    if \"deviceid\" in df.columns:\n",
    "        df.drop(columns=[\"deviceid\"], inplace=True)\n",
    "\n",
    "    # Interpolate missing values using time-based method\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    # Create time-based features\n",
    "    df[\"hour\"] = df.index.hour\n",
    "    df[\"sin_hour\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"cos_hour\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "    # df[\"day_of_week\"] = df.index.dayofweek\n",
    "    # df[\"sin_day\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "    # df[\"cos_day\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "\n",
    "    # Lag Features (Avoid NaNs)\n",
    "    df[\"pm2.5_lag1\"] = df[\"pm2.5cnc\"].shift(1).bfill()\n",
    "    df[\"pm2.5_lag2\"] = df[\"pm2.5cnc\"].shift(2).bfill()\n",
    "    df[\"pm10_lag1\"] = df[\"pm10cnc\"].shift(1).bfill()\n",
    "    df[\"pm10_lag2\"] = df[\"pm10cnc\"].shift(2).bfill()\n",
    "\n",
    "    # Month & Seasonal Encoding\n",
    "    df[\"month\"] = df.index.month\n",
    "\n",
    "    def get_season(month):\n",
    "        if month in [1, 2]:  \n",
    "            return \"Winter\"\n",
    "            return \"Summer (Pre-Monsoon)\"\n",
    "        elif month in [6, 7, 8, 9]:  \n",
    "            return \"Monsoon\"\n",
    "        else:  \n",
    "            return \"Post-Monsoon (Autumn)\"\n",
    "\n",
    "    df[\"season\"] = df[\"month\"].apply(get_season)\n",
    "    df[\"season_code\"] = pd.Categorical(df[\"season\"]).codes\n",
    "\n",
    "    # Weekend Indicator\n",
    "    # df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "    # Rate of Change Features\n",
    "    # df[\"pm2.5_diff\"] = df[\"pm2.5cnc\"].diff().fillna(0)  # Avoid NaNs\n",
    "    # df[\"pm10_diff\"] = df[\"pm10cnc\"].diff().fillna(0)  # Avoid NaNs\n",
    "\n",
    "    # Drop Non-Numeric Columns\n",
    "    df.drop(columns=[\"season\"], inplace=True)\n",
    "    # df.drop(columns=[\"hour\"], inplace=True)\n",
    "    # df.drop(columns=[\"minute\"], inplace=True)\n",
    "    \n",
    "\n",
    "    # Final NaN Handling (Ensures No NaNs Remain)\n",
    "    df.fillna(method=\"ffill\", inplace=True)  # Forward-fill\n",
    "    df.fillna(method=\"bfill\", inplace=True)  # Back-fill\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7ff80-7873-47c8-bcf4-bcf6e0b38db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------ Encoding Functions ------------------\n",
    "\n",
    "def encode_pm25(pm25):\n",
    "    \"\"\"\n",
    "    Encodes PM2.5 value into a numeric score:\n",
    "      1: Good (0–30 µg/m³)\n",
    "      2: Satisfactory (31–60 µg/m³)\n",
    "      3: Moderate (61–90 µg/m³)\n",
    "      4: Poor (91–120 µg/m³)\n",
    "      5: Very Poor (121–250 µg/m³)\n",
    "      6: Severe (251 µg/m³ and above)\n",
    "    \"\"\"\n",
    "    if pm25 <= 30:\n",
    "        return 1\n",
    "    elif pm25 <= 60:\n",
    "        return 2\n",
    "    elif pm25 <= 90:\n",
    "        return 3\n",
    "    elif pm25 <= 120:\n",
    "        return 4\n",
    "    elif pm25 <= 250:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6\n",
    "\n",
    "def encode_pm10(pm10):\n",
    "    \"\"\"\n",
    "    Encodes PM10 value into a numeric score:\n",
    "      1: Good (0–50 µg/m³)\n",
    "      2: Satisfactory (51–100 µg/m³)\n",
    "      3: Moderately Polluted (101–250 µg/m³)\n",
    "      4: Poor (251–350 µg/m³)\n",
    "      5: Very Poor (351–430 µg/m³)\n",
    "      6: Severe (431 µg/m³ and above)\n",
    "    \"\"\"\n",
    "    if pm10 <= 50:\n",
    "        return 1\n",
    "    elif pm10 <= 100:\n",
    "        return 2\n",
    "    elif pm10 <= 250:\n",
    "        return 3\n",
    "    elif pm10 <= 350:\n",
    "        return 4\n",
    "    elif pm10 <= 430:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6\n",
    "\n",
    "def combined_air_quality_label(pm25, pm10):\n",
    "    \"\"\"\n",
    "    Combines PM2.5 and PM10 numeric scores by taking the floor of the average.\n",
    "    For example, if one pollutant is 'Severe' (6) and the other 'Moderate' (3),\n",
    "    the average (4.5) is floored to 4, which we interpret as 'Poor'.\n",
    "    \"\"\"\n",
    "    score_pm25 = encode_pm25(pm25)\n",
    "    score_pm10 = encode_pm10(pm10)\n",
    "    # Floor the average using integer division (works for positive numbers)\n",
    "    combined_score = (score_pm25 + score_pm10) // 2\n",
    "    return combined_score\n",
    "\n",
    "# Optional mapping back to text label if needed\n",
    "combined_labels = {\n",
    "    1: \"Good\",\n",
    "    2: \"Satisfactory\",\n",
    "    3: \"Moderate\",\n",
    "    4: \"Poor\",\n",
    "    5: \"Very Poor\",\n",
    "    6: \"Severe\"\n",
    "}\n",
    "\n",
    "def process_air_quality_data(df):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame containing 'pm25' and 'pm10' columns:\n",
    "      - Encodes each pollutant into its numeric score.\n",
    "      - Creates a combined score.\n",
    "      - Retains only numeric columns.\n",
    "    \"\"\"\n",
    "    df['pm25_numeric'] = df['pm25'].apply(encode_pm25)\n",
    "    df['pm10_numeric'] = df['pm10'].apply(encode_pm10)\n",
    "    df['combined_numeric'] = df.apply(\n",
    "        lambda row: combined_air_quality_label(row['pm25'], row['pm10']),\n",
    "        axis=1\n",
    "    )\n",
    "    df['combined_label'] = df['combined_numeric'].map(combined_labels)\n",
    "    \n",
    "    # Keep only numeric columns (if desired)\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    return df_numeric\n",
    "\n",
    "# ------------------ Misclassification Identification ------------------\n",
    "\n",
    "def check_misclassification(row, pollutant):\n",
    "    \"\"\"\n",
    "    Identifies if the anomaly detection for a row is a misclassification.\n",
    "    \n",
    "    Logic:\n",
    "      - Computes the combined numeric air quality label using both 'pm2.5cnc' and 'pm10cnc' columns.\n",
    "      - If an anomaly is flagged (column value 1) but the combined label is in a milder category (1, 2, or 3),\n",
    "        it is a misclassification.\n",
    "      - If no anomaly is flagged (0) but the combined label is in a worse category (4, 5, or 6),\n",
    "        it is a misclassification.\n",
    "    \n",
    "    Returns:\n",
    "      - True if misclassification, False otherwise.\n",
    "    \"\"\"\n",
    "    # Compute combined numeric label from the pollutant values (assumes both columns are available)\n",
    "    combined = combined_air_quality_label(row[\"pm2.5cnc\"], row[\"pm10cnc\"])\n",
    "    \n",
    "    # Determine the anomaly flag column (e.g., \"Anomaly_pm25cnc_features\" for \"pm2.5cnc\")\n",
    "    anomaly_col = f\"Anomaly_{pollutant.replace('.', '')}\"\n",
    "    anomaly_flag = row[anomaly_col]\n",
    "    \n",
    "    if anomaly_flag == 1 and combined in [1, 2, 3]:\n",
    "        return True\n",
    "    # elif anomaly_flag == 0 and combined in [4, 5, 6]:\n",
    "    #     return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdcd84b-ea51-4c87-abf2-c20750160dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_isoforest(train_data):\n",
    "    \"\"\"\n",
    "    Trains an Isolation Forest on the given training data.\n",
    "    Converts detected anomalies (-1) to normal (1) only if they fall within safe AQI ranges:\n",
    "      - For 'pm2.5cnc': safe range is 0 to 90.\n",
    "      - For 'pm10cnc': safe range is 0 to 350.\n",
    "    Returns: (model, scores, labels)\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "    model.fit(train_data)\n",
    "    \n",
    "    # Get outputs from Isolation Forest\n",
    "    scores = model.decision_function(train_data)  # Higher => more normal\n",
    "    labels = model.predict(train_data)              # 1 => normal, -1 => anomaly\n",
    "\n",
    "    # Define safe range mask based on available pollutant columns\n",
    "    # safe_range_mask = None\n",
    "    # if 'pm2.5cnc' in train_data.columns and 'pm10cnc' in train_data.columns:\n",
    "    #     mask_pm25 = (train_data[\"pm2.5cnc\"] >= 0) & (train_data[\"pm2.5cnc\"] <= 90)\n",
    "    #     mask_pm10 = (train_data[\"pm10cnc\"] >= 0) & (train_data[\"pm10cnc\"] <= 350)\n",
    "    #     safe_range_mask = mask_pm25 & mask_pm10\n",
    "    # elif 'pm2.5cnc' in train_data.columns:\n",
    "    #     safe_range_mask = (train_data[\"pm2.5cnc\"] >= 0) & (train_data[\"pm2.5cnc\"] <= 90)\n",
    "    # elif 'pm10cnc' in train_data.columns:\n",
    "    #     safe_range_mask = (train_data[\"pm10cnc\"] >= 0) & (train_data[\"pm10cnc\"] <= 350)\n",
    "    \n",
    "    # # For rows flagged as anomaly (-1) but within the safe range, force them to normal (1)\n",
    "    # if safe_range_mask is not None:\n",
    "    #     labels = np.where((labels == -1) & safe_range_mask, 1, labels)\n",
    "    \n",
    "    return model, scores, labels\n",
    "\n",
    "\n",
    "def run_svm(train_data):\n",
    "    \"\"\"\n",
    "    Trains a One-Class SVM on the given training data.\n",
    "    Converts detected anomalies (-1) to normal (1) only if they fall within safe AQI ranges:\n",
    "      - For 'pm2.5cnc': safe range is 0 to 90.\n",
    "      - For 'pm10cnc': safe range is 0 to 350.\n",
    "    Returns: (model, scores, labels)\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model = OneClassSVM(nu=0.05, kernel=\"rbf\", gamma=\"scale\")\n",
    "    model.fit(train_data)\n",
    "    \n",
    "    # Get outputs from One-Class SVM\n",
    "    scores = model.decision_function(train_data)  # Higher => more normal\n",
    "    labels = model.predict(train_data)              # 1 => normal, -1 => anomaly\n",
    "\n",
    "    # Define safe range mask based on available pollutant columns\n",
    "    safe_range_mask = None\n",
    "    if 'pm2.5cnc' in train_data.columns and 'pm10cnc' in train_data.columns:\n",
    "        mask_pm25 = (train_data[\"pm2.5cnc\"] >= 0) & (train_data[\"pm2.5cnc\"] <= 250)\n",
    "        mask_pm10 = (train_data[\"pm10cnc\"] >= 0) & (train_data[\"pm10cnc\"] <= 350)\n",
    "        safe_range_mask = mask_pm25 & mask_pm10\n",
    "    elif 'pm2.5cnc' in train_data.columns:\n",
    "        safe_range_mask = (train_data[\"pm2.5cnc\"] >= 0) & (train_data[\"pm2.5cnc\"] <= 250)\n",
    "    elif 'pm10cnc' in train_data.columns:\n",
    "        safe_range_mask = (train_data[\"pm10cnc\"] >= 0) & (train_data[\"pm10cnc\"] <= 350)\n",
    "    \n",
    "    # For rows flagged as anomaly (-1) but within the safe range, force them to normal (1)\n",
    "    if safe_range_mask is not None:\n",
    "        labels = np.where((labels == -1) & safe_range_mask, 1, labels)\n",
    "    \n",
    "    return model, scores, labels\n",
    "\n",
    "def split_data_random(df):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train (70%), test (20%), and validation (10%).\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to split.\n",
    "\n",
    "    Returns:\n",
    "    train (DataFrame): 70% of data for training.\n",
    "    test (DataFrame): 20% of data for testing.\n",
    "    validation (DataFrame): 10% of data for final validation.\n",
    "    \"\"\"\n",
    "    train, temp = train_test_split(df, test_size=0.3, random_state=42)  # 70% train, 30% temp\n",
    "    test, validation = train_test_split(temp, test_size=1/3, random_state=42)  # 20% test, 10% validation\n",
    "    return train, test, validation\n",
    "\n",
    "def split_data(df):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train (70%), test (20%), and validation (10%) \n",
    "    in a historical order (earliest to latest).\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to split. Assumes it is sorted by timestamp.\n",
    "    \n",
    "    Returns:\n",
    "    train (DataFrame): First 70% of data for training.\n",
    "    test (DataFrame): Next 20% of data for testing.\n",
    "    validation (DataFrame): Final 10% of data for validation.\n",
    "    \"\"\"\n",
    "    # Ensure data is sorted by timestamp (assuming a 'timestamp' column exists)\n",
    "    df = df.sort_values(by='dt_time')\n",
    "\n",
    "    # Compute split indices\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.7)\n",
    "    test_end = int(n * 0.9)  # 70% train + 20% test = 90%, remaining 10% is validation\n",
    "\n",
    "    # Split sequentially\n",
    "    train = df.iloc[:train_end]\n",
    "    test = df.iloc[train_end:test_end]\n",
    "    validation = df.iloc[test_end:]\n",
    "\n",
    "    return train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8795c-5d21-4a10-9f79-262654094012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zoomed_anomalies(df, start_date, end_date, pollutant):\n",
    "    # List possible column name formats for the anomaly column\n",
    "    possible_names = [\n",
    "        f\"Anomaly_{pollutant}_features\",\n",
    "        f\"Anomaly_{pollutant.replace('.', '')}_features\",\n",
    "        f\"Anomaly_{pollutant}\",\n",
    "        f\"Anomaly_{pollutant.replace('.', '')}\"\n",
    "    ]\n",
    "    \n",
    "    anomaly_col = None\n",
    "    for name in possible_names:\n",
    "        if name in df.columns:\n",
    "            anomaly_col = name\n",
    "            break\n",
    "\n",
    "    if anomaly_col is None:\n",
    "        raise KeyError(f\"No anomaly column found for pollutant '{pollutant}'. Available columns: {df.columns}\")\n",
    "    \n",
    "    # Filter DataFrame for the specified date range\n",
    "    df_filtered = df.loc[start_date:end_date]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_filtered.index, df_filtered[pollutant], label=pollutant, color='blue')\n",
    "    \n",
    "    # Highlight anomalies using the found anomaly column\n",
    "    anomalies = df_filtered[df_filtered[anomaly_col] == 1]\n",
    "    plt.scatter(anomalies.index, anomalies[pollutant], color='red', label=\"Anomaly\", marker='o')\n",
    "    \n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(f\"Zoomed Anomalies for {pollutant}\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e517506e-03fc-4ffa-82e5-c561fbae3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_anomalies(df, model_type, pollutant):\n",
    "    \"\"\"\n",
    "    Generic anomaly detection pipeline for PM2.5 or PM10.\n",
    "\n",
    "    Parameters:\n",
    "      - df: DataFrame containing the air quality data.\n",
    "      - model_type: One of {\"svm\", \"iforest\", \"gmm\", \"dbscan\", \"lstm\"}.\n",
    "      - pollutant: Either \"pm2.5cnc\" or \"pm10cnc\".\n",
    "\n",
    "    Returns:\n",
    "      - Updated DataFrame with anomaly and misclassification labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure valid pollutant name\n",
    "    if pollutant not in [\"pm2.5cnc\", \"pm10cnc\"]:\n",
    "        raise ValueError(\"Invalid pollutant. Choose 'pm2.5cnc' or 'pm10cnc'.\")\n",
    "    \n",
    "    # Define the anomaly column name (e.g., \"Anomaly_pm25cnc\")\n",
    "    anomaly_col = f\"Anomaly_{pollutant.replace('.', '')}\"\n",
    "    \n",
    "    # Data Cleaning & Preparation\n",
    "    df = clean_and_prepare(df)\n",
    "    train, test, validation = split_data_random(df)\n",
    "    \n",
    "    # Select the correct model function\n",
    "    model_functions = {\n",
    "        \"svm\": run_svm,\n",
    "        \"iforest\": run_isoforest,\n",
    "        # Additional models can be added here as needed.\n",
    "    }\n",
    "    \n",
    "    if model_type not in model_functions:\n",
    "        raise ValueError(\"Invalid model_type. Choose from 'svm', 'iforest', 'gmm', 'dbscan', 'lstm'.\")\n",
    "    \n",
    "    # Extract the pollutant column for anomaly detection\n",
    "    train_data = train[[pollutant]]\n",
    "    test_data = test[[pollutant]]\n",
    "    val_data = validation[[pollutant]]\n",
    "    \n",
    "    # Run anomaly detection model on train, test, and validation splits\n",
    "    model, train_scores, train_labels = model_functions[model_type](train_data)\n",
    "    _, test_scores, test_labels = model_functions[model_type](test_data)\n",
    "    _, val_scores, val_labels = model_functions[model_type](val_data)\n",
    "    # print(\"Validation labels:\", val_labels)\n",
    "    \n",
    "    # Store anomaly labels (1 = anomaly, 0 = normal)\n",
    "    train[\"Anomaly\"] = (train_labels == -1).astype(int)\n",
    "    test[\"Anomaly\"] = (test_labels == -1).astype(int)\n",
    "    validation[\"Anomaly\"] = (val_labels == -1).astype(int)\n",
    "    \n",
    "    # Merge anomaly labels back into the original DataFrame\n",
    "    df[anomaly_col] = 0  # Default: no anomaly\n",
    "    df.loc[train.index, anomaly_col] = train[\"Anomaly\"]\n",
    "    df.loc[test.index, anomaly_col] = test[\"Anomaly\"]\n",
    "    df.loc[validation.index, anomaly_col] = validation[\"Anomaly\"]\n",
    "    \n",
    "    \n",
    "    # Integrate misclassification identification\n",
    "    misclass_col = f\"Misclassification_{pollutant.replace('.', '')}\"\n",
    "    df[misclass_col] = df.apply(lambda row: check_misclassification(row, pollutant), axis=1)\n",
    "    \n",
    "    # # Create ground truth anomaly labels: rows with combinedlabels in [4, 5, 6] are true outliers\n",
    "    # if 'combined_label' in df.columns:\n",
    "    #     df[\"True_Anomaly\"] = df[\"combined_label\"].apply(lambda x: 1 if x in [4, 5, 6] else 0)\n",
    "    # else:\n",
    "    #     raise ValueError(\"DataFrame must contain 'combinedlabels' column for ground truth anomalies\")\n",
    "    \n",
    "    # # Calculate metrics using the predicted labels from IsoForest and the ground truth\n",
    "    # from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    # accuracy = accuracy_score(df[\"True_Anomaly\"], df[\"Anomaly_IsoForest\"])\n",
    "    # precision = precision_score(df[\"True_Anomaly\"], df[\"Anomaly_IsoForest\"])\n",
    "    # recall = recall_score(df[\"True_Anomaly\"], df[\"Anomaly_IsoForest\"])\n",
    "    # f1 = f1_score(df[\"True_Anomaly\"], df[\"Anomaly_IsoForest\"])\n",
    "    \n",
    "    # print(\"Accuracy:\", accuracy)\n",
    "    # print(\"Precision:\", precision)\n",
    "    # print(\"Recall:\", recall)\n",
    "    # print(\"F1 Score:\", f1)\n",
    "    \n",
    "    # ------------------ Plotting ------------------\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.scatter(train.index, train[pollutant], c=train[\"Anomaly\"], cmap=\"coolwarm\", label=\"Train\", s=15)\n",
    "    # You can uncomment these lines if you wish to plot test and validation splits as well.\n",
    "    # plt.scatter(test.index, test[pollutant], c=test[\"Anomaly\"], cmap=\"coolwarm\", label=\"Test\", s=15, marker=\"x\")\n",
    "    # plt.scatter(validation.index, validation[pollutant], c=validation[\"Anomaly\"], cmap=\"coolwarm\", label=\"Validation\", s=15, marker=\"^\")\n",
    "    plt.title(f\"{pollutant.upper()} Concentration Anomalies ({model_type.upper()})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(f\"{pollutant} Concentration\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print anomaly counts for each split\n",
    "    print(f\"Train anomalies ({pollutant}): {train['Anomaly'].sum()}\")\n",
    "    print(f\"Test anomalies ({pollutant}): {test['Anomaly'].sum()}\")\n",
    "    print(f\"Validation anomalies ({pollutant}): {validation['Anomaly'].sum()}\")\n",
    "    \n",
    "    # Print misclassification count\n",
    "    total_misclassified = df[misclass_col].sum()\n",
    "    print(f\"Total misclassifications for {pollutant}: {total_misclassified}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3092c0e-00f0-41fe-b7e6-3e2585cdf36e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33msite_104_data.csv\u001b[39m\u001b[33m\"\u001b[39m, parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mdt_time\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"site_104_data.csv\", parse_dates=[\"dt_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91788429-dec8-4666-8062-857c873ee480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df25svm = process_and_plot_anomalies(df, model_type=\"svm\", pollutant=\"pm2.5cnc\")\n",
    "# df10svm = process_and_plot_anomalies(df,model_type = \"svm\",pollutant=\"pm10cnc\")\n",
    "df10iforest = process_and_plot_anomalies(df, model_type=\"iforest\", pollutant=\"pm10cnc\")\n",
    "df25iforest = process_and_plot_anomalies(df,model_type=\"iforest\",pollutant=\"pm2.5cnc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d142aa-e65a-4d04-9240-c91f1414e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df25iforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af97d7-b67d-47d2-b9f2-965b27331ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    df10iforest[['pm10cnc', 'Anomaly_pm10cnc']], \n",
    "    df25iforest[['pm2.5cnc', 'Anomaly_pm25cnc']], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Reset index if necessary\n",
    "merged_df.reset_index(inplace=True)\n",
    "\n",
    "# Display the final dataframe\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df751b15-54f1-4b0a-b081-4ac35b232bb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to generate anomaly JSON data\n",
    "def get_anomaly_data(df25, df10, site_id):\n",
    "    # Drop unnecessary columns\n",
    "    df10 = df10.drop(columns=[\"index\", \"level_0\"], errors=\"ignore\")\n",
    "    df25 = df25.drop(columns=[\"index\", \"level_0\"], errors=\"ignore\")\n",
    "\n",
    "    # Set 'dt_time' as index\n",
    "    df10.set_index(\"dt_time\", inplace=True)\n",
    "    df25.set_index(\"dt_time\", inplace=True)\n",
    "\n",
    "    # Merge both DataFrames on 'dt_time' (datetime index)\n",
    "    merged_df = pd.merge(df10, df25, on=\"dt_time\", how=\"inner\")\n",
    "\n",
    "    # Reset index for JSON output & convert dt_time to string\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df[\"dt_time\"] = merged_df[\"dt_time\"].astype(str)\n",
    "\n",
    "    # Return JSON formatted data for given site_id\n",
    "    return {site_id: merged_df.to_dict(orient=\"records\")}\n",
    "\n",
    "# Function to save JSON data without overwriting existing keys\n",
    "def jsonify(data, filename=\"output.json\"):\n",
    "    # Load existing data if file exists\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    else:\n",
    "        existing_data = {}\n",
    "\n",
    "    # Update with new site_id data without deleting old ones\n",
    "    existing_data.update(data)\n",
    "\n",
    "    # Save updated JSON\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(existing_data, f, indent=4)  # Pretty-print JSON\n",
    "\n",
    "# Example usage: Add new data for site_id 104 while keeping 117\n",
    "jsonify(get_anomaly_data(df25iforest, df10iforest, 104))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89166575-1f2f-4e30-86a9-160bf36adece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10iforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5616dc-8894-4e70-8e92-c1773b462619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_and_plot_anomalies(df, model_type, pollutant):\n",
    "#     \"\"\"\n",
    "#     Generic anomaly detection pipeline for PM2.5 or PM10.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df: DataFrame containing the air quality data.\n",
    "#     - model_type: One of {\"svm\", \"iforest\", \"gmm\", \"dbscan\", \"lstm\"}.\n",
    "#     - pollutant: Either \"pm2.5cnc\" or \"pm10cnc\".\n",
    "\n",
    "#     Returns:\n",
    "#     - Updated DataFrame with anomaly labels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Ensure valid pollutant name\n",
    "#     if pollutant not in [\"pm2.5cnc\", \"pm10cnc\"]:\n",
    "#         raise ValueError(\"Invalid pollutant. Choose 'pm2.5cnc' or 'pm10cnc'.\")\n",
    "\n",
    "#     # Mapping of pollutant to its anomaly column\n",
    "#     anomaly_col = f\"Anomaly_{pollutant.replace('.', '')}\"\n",
    "\n",
    "#     # Data Cleaning & Preparation\n",
    "#     df = clean_and_prepare(df)\n",
    "#     train, test, validation = split_data(df)\n",
    "\n",
    "#     # Select the correct model function\n",
    "#     model_functions = {\n",
    "#         \"svm\": run_svm,\n",
    "#         \"iforest\": run_isoforest,\n",
    "#     }\n",
    "\n",
    "#     if model_type not in model_functions:\n",
    "#         raise ValueError(\"Invalid model_type. Choose from 'svm', 'iforest', 'gmm', 'dbscan', 'lstm'.\")\n",
    "\n",
    "#     # Extract only the pollutant column for anomaly detection\n",
    "#     train_data = train[[pollutant]]\n",
    "#     test_data = test[[pollutant]]\n",
    "#     val_data = validation[[pollutant]]\n",
    "\n",
    "#     # Run anomaly detection model\n",
    "#     model, train_scores, train_labels = model_functions[model_type](train_data)\n",
    "#     _, test_scores, test_labels = model_functions[model_type](test_data)\n",
    "#     _, val_scores, val_labels = model_functions[model_type](val_data)\n",
    "\n",
    "#     # Store anomaly labels\n",
    "#     train[\"Anomaly\"] = (train_labels == -1).astype(int)\n",
    "#     test[\"Anomaly\"] = (test_labels == -1).astype(int)\n",
    "#     validation[\"Anomaly\"] = (val_labels == -1).astype(int)\n",
    "\n",
    "#     # Merge back into the original dataframe\n",
    "#     df[anomaly_col] = 0  # Default: no anomaly\n",
    "#     df.loc[train.index, anomaly_col] = train[\"Anomaly\"]\n",
    "#     df.loc[test.index, anomaly_col] = test[\"Anomaly\"]\n",
    "#     df.loc[validation.index, anomaly_col] = validation[\"Anomaly\"]\n",
    "\n",
    "#     # Plot anomalies\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.scatter(train.index, train[pollutant], c=train[\"Anomaly\"], cmap=\"coolwarm\", label=\"Train\", s=15)\n",
    "#     plt.scatter(test.index, test[pollutant], c=test[\"Anomaly\"], cmap=\"coolwarm\", label=\"Test\", s=15, marker=\"x\")\n",
    "#     plt.scatter(validation.index, validation[pollutant], c=validation[\"Anomaly\"], cmap=\"coolwarm\", label=\"Validation\", s=15, marker=\"^\")\n",
    "#     plt.title(f\"{pollutant.upper()} Concentration Anomalies ({model_type.upper()})\")\n",
    "#     plt.xlabel(\"Time\")\n",
    "#     plt.ylabel(f\"{pollutant} Concentration\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Print anomaly counts\n",
    "#     print(f\"Train anomalies ({pollutant}): {train['Anomaly'].sum()}\")\n",
    "#     print(f\"Test anomalies ({pollutant}): {test['Anomaly'].sum()}\")\n",
    "#     print(f\"Validation anomalies ({pollutant}): {validation['Anomaly'].sum()}\")\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b07655-0f55-4f2a-9370-aaf3ba42fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def check_misclassification_single(row, pollutant):\n",
    "#     \"\"\"\n",
    "#     Identifies if the anomaly detection for a given row is a misclassification.\n",
    "    \n",
    "#     Logic for a single pollutant:\n",
    "#       - For PM2.5:\n",
    "#           * If anomaly flagged (==1) but value encodes to 1-3 (mild), then misclassified.\n",
    "#           * If no anomaly (==0) but value encodes to 4-6 (severe), then misclassified.\n",
    "#       - For PM10:\n",
    "#           * If anomaly flagged (==1) but value encodes to 1-3 (mild), then misclassified.\n",
    "#           * If no anomaly (==0) but value encodes to 4-6 (severe), then misclassified.\n",
    "#     \"\"\"\n",
    "#     # Determine encoded severity based on pollutant type.\n",
    "#     if pollutant == \"pm2.5cnc\":\n",
    "#         encoded = encode_pm25(row[pollutant])\n",
    "#     elif pollutant == \"pm10cnc\":\n",
    "#         encoded = encode_pm10(row[pollutant])\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid pollutant. Choose 'pm2.5cnc' or 'pm10cnc'.\")\n",
    "    \n",
    "#     # Define ranges: 1-3 (mild) and 4-6 (severe)\n",
    "#     is_anomaly = row[f\"Anomaly_{pollutant.replace('.', '')}\"]\n",
    "#     if is_anomaly == 1 and encoded in [1, 2, 3]:\n",
    "#         return True\n",
    "#     # elif is_anomaly == 0 and encoded in [4, 5, 6]:\n",
    "#     #     return True\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "# # ------------------ Anomaly Detection Pipeline ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2b375-7ec7-42fc-bfd7-d370e09697c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_and_plot_with_features(df, model_type, pollutant):\n",
    "#     \"\"\"\n",
    "#     Anomaly detection pipeline using additional features from clean_and_prepare.\n",
    "    \n",
    "#     Parameters:\n",
    "#       - df: DataFrame containing the air quality data.\n",
    "#       - model_type: One of {\"svm\", \"iforest\", \"gmm\", \"dbscan\", \"lstm\"}.\n",
    "#       - pollutant: Either \"pm2.5cnc\" or \"pm10cnc\".\n",
    "      \n",
    "#     Returns:\n",
    "#       - Updated DataFrame with anomaly and misclassification labels.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Ensure valid pollutant selection\n",
    "#     if pollutant not in [\"pm2.5cnc\", \"pm10cnc\"]:\n",
    "#         raise ValueError(\"Invalid pollutant. Choose 'pm2.5cnc' or 'pm10cnc'.\")\n",
    "    \n",
    "#     # Construct the anomaly column name (remove '.' from pollutant name)\n",
    "#     anomaly_col = f\"Anomaly_{pollutant.replace('.', '')}_features\"\n",
    "    \n",
    "#     # Apply feature engineering (assumed to be defined elsewhere)\n",
    "#     df = clean_and_prepare(df)\n",
    "    \n",
    "#     # Train-Test-Validation Split (assumed to be defined elsewhere)\n",
    "#     train, test, validation = split_data(df)\n",
    "    \n",
    "#     # Select features: all numerical columns except timestamp and the pollutant itself\n",
    "#     feature_columns = [col for col in df.columns if col not in [\"dt_time\", pollutant]]\n",
    "    \n",
    "#     # Extract feature matrices\n",
    "#     train_X, test_X, val_X = train[feature_columns], test[feature_columns], validation[feature_columns]\n",
    "    \n",
    "#     # Define available model functions (assumed to be defined elsewhere)\n",
    "#     model_functions = {\n",
    "#         \"svm\": run_svm,\n",
    "#         \"iforest\": run_isoforest,\n",
    "#         # \"gmm\": run_gmm,\n",
    "#         # \"dbscan\": run_dbscan,\n",
    "#         # \"lstm\": run_lstm_autoencoder\n",
    "#     }\n",
    "    \n",
    "#     if model_type not in model_functions:\n",
    "#         raise ValueError(\"Invalid model_type. Choose from 'svm', 'iforest', 'gmm', 'dbscan', 'lstm'.\")\n",
    "    \n",
    "#     # Run the anomaly detection model on train, test, and validation splits\n",
    "#     model, train_scores, train_labels = model_functions[model_type](train_X)\n",
    "#     _, test_scores, test_labels = model_functions[model_type](test_X)\n",
    "#     _, val_scores, val_labels = model_functions[model_type](val_X)\n",
    "    \n",
    "#     # Convert labels to binary (1 = anomaly, 0 = normal)\n",
    "#     train[\"Anomaly\"] = (train_labels == -1).astype(int)\n",
    "#     test[\"Anomaly\"] = (test_labels == -1).astype(int)\n",
    "#     validation[\"Anomaly\"] = (val_labels == -1).astype(int)\n",
    "    \n",
    "#     # Merge anomaly results back into the original DataFrame\n",
    "#     df[anomaly_col] = 0  # Default: no anomaly\n",
    "#     df.loc[train.index, anomaly_col] = train[\"Anomaly\"]\n",
    "#     df.loc[test.index, anomaly_col] = test[\"Anomaly\"]\n",
    "#     df.loc[validation.index, anomaly_col] = validation[\"Anomaly\"]\n",
    "    \n",
    "#     # --- Integrate Misclassification Identification ---\n",
    "#     # Add a new column to mark misclassifications based on the logic provided.\n",
    "#     misclass_col = f\"Misclassification_{pollutant.replace('.', '')}_features\"\n",
    "#     df[misclass_col] = df.apply(lambda row: check_misclassification(row, pollutant), axis=1)\n",
    "    \n",
    "#     # ------------------ Plotting ------------------\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.scatter(train.index, train[pollutant], c=train[\"Anomaly\"], cmap=\"coolwarm\", label=\"Train\", s=15)\n",
    "#     plt.scatter(test.index, test[pollutant], c=test[\"Anomaly\"], cmap=\"coolwarm\", label=\"Test\", s=15, marker=\"x\")\n",
    "#     plt.scatter(validation.index, validation[pollutant], c=validation[\"Anomaly\"], cmap=\"coolwarm\", label=\"Validation\", s=15, marker=\"^\")\n",
    "#     plt.title(f\"{pollutant.upper()} Concentration Anomalies with Features ({model_type.upper()})\")\n",
    "#     plt.xlabel(\"Time\")\n",
    "#     plt.ylabel(f\"{pollutant} Concentration\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Print anomaly counts\n",
    "#     print(f\"Train anomalies ({pollutant} + features): {train['Anomaly'].sum()}\")\n",
    "#     print(f\"Test anomalies ({pollutant} + features): {test['Anomaly'].sum()}\")\n",
    "#     print(f\"Validation anomalies ({pollutant} + features): {validation['Anomaly'].sum()}\")\n",
    "    \n",
    "#     # Print misclassification counts\n",
    "#     total_misclassified = df[misclass_col].sum()\n",
    "#     print(f\"Total misclassifications for {pollutant} with features: {total_misclassified}\")\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78647922-06ce-4b1b-a415-9827f7f2e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "# X = df[numeric_cols].values\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Apply PCA with all components to see full variance structure\n",
    "# pca = PCA(n_components=X_scaled.shape[1])\n",
    "# pca.fit(X_scaled)\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "# cum_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# # ------------------ Plot Cumulative Explained Variance (Elbow Plot) ------------------\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.plot(np.arange(1, len(cum_explained_variance) + 1), cum_explained_variance, marker='o', linestyle='--')\n",
    "# plt.xlabel(\"Number of Components\")\n",
    "# plt.ylabel(\"Cumulative Explained Variance\")\n",
    "# plt.title(\"Cumulative Explained Variance vs. Number of PCA Components\")\n",
    "# plt.axhline(y=0.90, color='red', linestyle='-')\n",
    "# plt.text(1, 0.85, \"90% Threshold\", color='red', fontsize=12)\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # ------------------ Plot Scree Plot ------------------\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.bar(np.arange(1, len(explained_variance) + 1), explained_variance, color='skyblue')\n",
    "# plt.xlabel(\"Principal Component\")\n",
    "# plt.ylabel(\"Explained Variance Ratio\")\n",
    "# plt.title(\"Scree Plot\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # ------------------ Print Out Variance and Feature Loadings ------------------\n",
    "\n",
    "# print(\"Cumulative Explained Variance by Component:\")\n",
    "# for i, cum_var in enumerate(cum_explained_variance, start=1):\n",
    "#     print(f\"Component {i}: {cum_var:.2f}\")\n",
    "\n",
    "# # Create a DataFrame for loadings to inspect which features contribute most\n",
    "# loadings = pca.components_.T\n",
    "# loading_df = pd.DataFrame(loadings, index=numeric_cols,\n",
    "#                           columns=[f\"PC{i}\" for i in range(1, len(explained_variance)+1)])\n",
    "# print(\"\\nFeature Loadings for each Principal Component:\")\n",
    "# print(loading_df)\n",
    "\n",
    "# # Optionally, you can visually inspect loadings for the first few components:\n",
    "# # For example, a bar plot for PC1 loadings:\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# pc1_loadings = loading_df[\"PC1\"].abs().sort_values(ascending=False)\n",
    "# pc1_loadings.plot(kind='bar', color='green')\n",
    "# plt.xlabel(\"Features\")\n",
    "# plt.ylabel(\"Absolute Loading on PC1\")\n",
    "# plt.title(\"Feature Contribution to PC1\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9613d-2674-4a8c-8e53-9902c3b89d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04d2ec-d501-4128-b5e8-43eea43bd664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
